@inproceedings{toshniwal_stormtwitter_2014,
  address = {New York, {NY}, {USA}},
  series = {{SIGMOD} '14},
  title = {Storm@{T}witter},
  isbn = {978-1-4503-2376-5},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2588555.2595641},
  doi = {10.1145/2588555.2595641},
  abstract = {This paper describes the use of Storm at Twitter. Storm is a real-time fault-tolerant and distributed stream data processing system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper describes the architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work.},
  urldate = {2014-08-11},
  booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} International Conference on Management of Data},
  publisher = {{ACM}},
  author = {Toshniwal, Ankit and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy},
  year = {2014},
  keywords = {real-time query processing, stream data management},
  pages = {147–156}
}

@incollection{bohlouli_towards_2013,
  title = {Towards an integrated platform for big data analysis},
  url = {http://link.springer.com.ezproxy.lib.monash.edu.au/chapter/10.1007/978-3-642-34471-8_4},
  urldate = {2014-08-25},
  booktitle = {Integration of Practice-Oriented Knowledge Technology: Trends and Prospectives},
  publisher = {Springer},
  author = {Bohlouli, Mahdi and Schulz, Frank and Angelis, Lefteris and Pahor, David and Brandic, Ivona and Atlan, David and Tate, Rosemary},
  year = {2013},
  pages = {47--56}
}

@misc{web:Storm,
  title = {Storm, distributed and fault-tolerant realtime computation},
  year = {2013},
  month = {November},
  howpublished = {\mbox{\url{http://storm-project.net}}}
}

@misc{web:synchrotron,
  title = {Australian {S}ynchrotron},
  year = {2014},
  month = {August},
  howpublished = {\mbox{\url{https://www.synchrotron.org.au}}}
}

@misc{web:LHC,
  title = {The {L}arge {H}adron {C}ollider | {CERN}},
  year = {2014},
  month = {August},
  howpublished = {\mbox{\url{http://home.web.cern.ch/topics/large-hadron-collider}}}
}

@misc{web:Amazon,
  title = {Amazon.com: {O}nline {S}hopping for {E}lectronics, {A}pparel, {C}omputers, {B}ooks, {DVD}s \& more},
  year = {2014},
  month = {August},
  howpublished = {\mbox{\url{http://www.amazon.com}}}
}

@misc{web:SparkStreaming,
  title = {Spark {S}treaming | {A}pache {S}park},
  year = {2014},
  howpublished = {\mbox{\url{https://spark.apache.org/streaming/}}}
}

@misc{web:UCBerkelyAMCLab,
  title = {AMPLab - {UC} {B}erkeley | {A}lgorithms, {M}achines and {P}eople {L}ab},
  year = {2014},
  howpublished = {\mbox{\url{https://amplab.cs.berkeley.edu}}}
}

@misc{web:Spark,
  title = {Apache {S}park\texttrademark -- {L}ightning-{F}ast {C}luster {C}omputing},
  year = {2014},
  howpublished = {\mbox{\url{https://spark.apache.org}}}
}

@inproceedings{liu_survey_2014,
  address = {New York, {NY}, {USA}},
  series = {{IDEAS} '14},
  title = {Survey of Real-time Processing Systems for Big Data},
  isbn = {978-1-4503-2627-8},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2628194.2628251},
  doi = {10.1145/2628194.2628251},
  urldate = {2014-08-25},
  booktitle = {Proceedings of the 18th International Database Engineering \&\#38; Applications Symposium},
  publisher = {{ACM}},
  author = {Liu, Xiufeng and Iftikhar, Nadeem and Xie, Xike},
  year = {2014},
  keywords = {architectures, big data, real-time, survey, systems},
  pages = {356--361}
}

@misc{web:Nectar,
  title = {home | {NeCTAR}},
  year = {2014},
  month = {August},
  howpublished = {\mbox{\url{http://www.nectar.org.au}}}
}

@inproceedings{klimt2004introducing,
  title={Introducing the Enron Corpus.},
  author={Klimt, Bryan and Yang, Yiming},
  booktitle={CEAS},
  year={2004}
}


@inproceedings{sinnott_towards_2011,
  title = {Towards an e-infrastructure for urban research across Australia},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6123291},
  urldate = {2014-08-30},
  booktitle = {E-Science (e-Science), 2011 {IEEE} 7th International Conference on},
  publisher = {{IEEE}},
  author = {Sinnott, Richard O. and Galang, Gerson and Tomko, Martin and Stimson, Robert},
  year = {2011},
  pages = {295--302},
  file = {[PDF] from unimelb.edu.au:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/5HT2S9FW/Sinnott et al. - 2011 - Towards an e-infrastructure for urban research acr.pdf:application/pdf;Snapshot:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/TU6W5AK5/login.html:text/html}
}

@book{marz2013principles,
  added-at = {2013-08-23T14:45:17.000+0200},
  address = {[S.l.]},
  author = {Marz, Nathan},
  biburl = {http://www.bibsonomy.org/bibtex/20dc290e810c709342f645179a9098cba/becker},
  description = {Big Data: Principles and Best Practices of Scalable Realtime Data Systems: Amazon.de: Nathan Marz, James Warren: Englische Bücher},
  interhash = {64a1336f5e196af365ef409e9ace3fa3},
  intrahash = {0dc290e810c709342f645179a9098cba},
  isbn = {1617290343 9781617290343},
  keywords = {big data lambda principles s:2013-08-23_sensorplatforms},
  publisher = {O'Reilly Media},
  refid = {810318817},
  timestamp = {2013-08-23T14:45:17.000+0200},
  title = {Big data : principles and best practices of scalable realtime data systems},
  url = {http://www.amazon.de/Big-Data-Principles-Practices-Scalable/dp/1617290343},
  year = 2013
}


@article{bifet_mining_2013,
  title = {Mining big data in real time},
  volume = {37},
  issn = {03505596},
  url = {http://go.galegroup.com/ps/i.do?id=GALE%7CA329730435&v=2.1&u=monash&it=r&p=AONE&sw=w&asid=43dd70afff364e9b2d73c1537dfe198a},
  language = {English},
  number = {1},
  urldate = {2014-08-11},
  journal = {Informatica},
  author = {Bifet, Albert},
  month = mar,
  year = {2013},
  keywords = {big data, Data mining},
  pages = {15+}
}


@inproceedings{ghemawat_google_2003,
  address = {New York, {NY}, {USA}},
  series = {{SOSP} '03},
  title = {The Google File System},
  isbn = {1-58113-757-5},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/945445.945450},
  doi = {10.1145/945445.945450},
  urldate = {2014-08-31},
  booktitle = {Proceedings of the Nineteenth {ACM} Symposium on Operating Systems Principles},
  publisher = {{ACM}},
  author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
  year = {2003},
  keywords = {clustered storage, data storage, fault tolerance, scalability},
  pages = {29--43}
}


@article{dean_mapreduce:_2008,
  title = {{MapReduce}: Simplified Data Processing on Large Clusters},
  volume = {51},
  issn = {0001-0782},
  shorttitle = {{MapReduce}},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1327452.1327492},
  doi = {10.1145/1327452.1327492},
  abstract = {{MapReduce} is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct {MapReduce} programs have been implemented internally at Google over the past four years, and an average of one hundred thousand {MapReduce} jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  number = {1},
  urldate = {2014-08-31},
  journal = {Commun. {ACM}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  month = jan,
  year = {2008},
  pages = {107--113}
}


@inproceedings{shvachko_hadoop_2010,
  title = {The Hadoop Distributed File System},
  doi = {10.1109/MSST.2010.5496972},
  abstract = {The Hadoop Distributed File System ({HDFS}) is designed to store very large data sets reliably, and to stream those data sets at high bandwidth to user applications. In a large cluster, thousands of servers both host directly attached storage and execute user application tasks. By distributing storage and computation across many servers, the resource can grow with demand while remaining economical at every size. We describe the architecture of {HDFS} and report on experience using {HDFS} to manage 25 petabytes of enterprise data at Yahoo!.},
  booktitle = {2010 {IEEE} 26th Symposium on Mass Storage Systems and Technologies ({MSST})},
  author = {Shvachko, K. and Kuang, Hairong and Radia, S. and Chansler, R.},
  month = may,
  year = {2010},
  keywords = {Bandwidth, Clustering algorithms, Computer architecture, Concurrent computing, data storage, data stream, Distributed computing, distributed databases, distributed file system, enterprise data, Facebook, File servers, File systems, Hadoop, Hadoop distributed file system, {HDFS}, Internet, network operating systems, Protection, Protocols, Yahoo!},
  pages = {1--10},
  file = {IEEE Xplore Abstract Record:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/5NSM8DN7/login.html:text/html}
}


@inproceedings{olston_pig_2008,
  address = {New York, {NY}, {USA}},
  series = {{SIGMOD} '08},
  title = {Pig Latin: A Not-so-foreign Language for Data Processing},
  isbn = {978-1-60558-102-6},
  shorttitle = {Pig Latin},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1376616.1376726},
  doi = {10.1145/1376616.1376726},
  abstract = {There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, {SQL} style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of {SQL}, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.},
  urldate = {2014-08-17},
  booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} International Conference on Management of Data},
  publisher = {{ACM}},
  author = {Olston, Christopher and Reed, Benjamin and Srivastava, Utkarsh and Kumar, Ravi and Tomkins, Andrew},
  year = {2008},
  keywords = {dataflow language, pig latin},
  pages = {1099--1110}
}


@inproceedings{thusoo_hive_2010,
  title = {Hive - a petabyte scale data warehouse using Hadoop},
  doi = {10.1109/ICDE.2010.5447738},
  abstract = {The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hadoop is a popular open-source map-reduce implementation which is being used in companies like Yahoo, Facebook etc. to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse. In this paper, we present Hive, an open-source data warehousing solution built on top of Hadoop. Hive supports queries expressed in a {SQL}-like declarative language - {HiveQL}, which are compiled into map-reduce jobs that are executed using Hadoop. In addition, {HiveQL} enables users to plug in custom map-reduce scripts into queries. The language includes a type system with support for tables containing primitive types, collections like arrays and maps, and nested compositions of the same. The underlying {IO} libraries can be extended to query data in custom formats. Hive also includes a system catalog - Metastore - that contains schemas and statistics, which are useful in data exploration, query optimization and query compilation. In Facebook, the Hive warehouse contains tens of thousands of tables and stores over 700TB of data and is being used extensively for both reporting and ad-hoc analyses by more than 200 users per month.},
  booktitle = {2010 {IEEE} 26th International Conference on Data Engineering ({ICDE})},
  author = {Thusoo, A and Sarma, J.S. and Jain, N. and Shao, Zheng and Chakka, P. and Zhang, Ning and Antony, S. and Liu, Hao and Murthy, R.},
  month = mar,
  year = {2010},
  keywords = {arrays, business intelligence, Companies, competitive intelligence, data exploration, data warehouses, Facebook, Hadoop software, Hardware, {HiveQL} language, Libraries, map-reduce jobs, maps, Metastore system catalog, nested compositions, open-source map-reduce implementation, Open source software, petabyte scale data warehouse, Plugs, primitive types, public domain software, query compilation, query optimization, query processing, {SQL}, {SQL}-like declarative language, Statistics, Warehousing},
  pages = {996--1005},
  file = {IEEE Xplore Abstract Record:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/JNB3RPAD/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/E5VJFX8C/Thusoo et al. - 2010 - Hive - a petabyte scale data warehouse using Hadoo.pdf:application/pdf}
}


@article{gates_building_2009,
  title = {Building a High-level Dataflow System on Top of Map-Reduce: The Pig Experience},
  volume = {2},
  issn = {2150-8097},
  shorttitle = {Building a High-level Dataflow System on Top of Map-Reduce},
  url = {http://dx.doi.org.ezproxy.lib.monash.edu.au/10.14778/1687553.1687568},
  doi = {10.14778/1687553.1687568},
  abstract = {Increasingly, organizations capture, transform and analyze enormous data sets. Prominent examples include internet companies and e-science. The Map-Reduce scalable dataflow paradigm has become popular for these applications. Its simple, explicit dataflow programming model is favored by some over the traditional high-level declarative approach: {SQL}. On the other hand, the extreme simplicity of Map-Reduce leads to much low-level hacking to deal with the many-step, branching dataflows that arise in practice. Moreover, users must repeatedly code standard operations such as join by hand. These practices waste time, introduce bugs, harm readability, and impede optimizations. Pig is a high-level dataflow system that aims at a sweet spot between {SQL} and Map-Reduce. Pig offers {SQL}-style high-level data manipulation constructs, which can be assembled in an explicit dataflow and interleaved with custom Map- and Reduce-style functions or executables. Pig programs are compiled into sequences of Map-Reduce jobs, and executed in the Hadoop Map-Reduce environment. Both Pig and Hadoop are open-source projects administered by the Apache Software Foundation. This paper describes the challenges we faced in developing Pig, and reports performance comparisons between Pig execution and raw Map-Reduce execution.},
  number = {2},
  urldate = {2014-08-31},
  journal = {Proc. {VLDB} Endow.},
  author = {Gates, Alan F. and Natkovich, Olga and Chopra, Shubham and Kamath, Pradeep and Narayanamurthy, Shravan M. and Olston, Christopher and Reed, Benjamin and Srinivasan, Santhosh and Srivastava, Utkarsh},
  month = aug,
  year = {2009},
  pages = {1414--1425}
}


@inproceedings{vavilapalli_apache_2013,
  address = {New York, {NY}, {USA}},
  series = {{SOCC} '13},
  title = {Apache Hadoop {YARN}: Yet Another Resource Negotiator},
  isbn = {978-1-4503-2428-1},
  shorttitle = {Apache Hadoop {YARN}},
  url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2523616.2523633},
  doi = {10.1145/2523616.2523633},
  abstract = {The initial design of Apache Hadoop [1] was tightly focused on running massive, {MapReduce} jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agorá---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the {MapReduce} programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler. In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: {YARN}. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running {YARN} on production environments (including 100\% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto {YARN} viz. Dryad, Giraph, Hoya, Hadoop {MapReduce}, {REEF}, Spark, Storm, Tez.},
  urldate = {2014-08-14},
  booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
  publisher = {{ACM}},
  author = {Vavilapalli, Vinod Kumar and Murthy, Arun C. and Douglas, Chris and Agarwal, Sharad and Konar, Mahadev and Evans, Robert and Graves, Thomas and Lowe, Jason and Shah, Hitesh and Seth, Siddharth and Saha, Bikas and Curino, Carlo and O'Malley, Owen and Radia, Sanjay and Reed, Benjamin and Baldeschwieler, Eric},
  year = {2013},
  pages = {5:1--5:16}
}


@article{harrison_hadoops_2012,
  title = {Hadoop's Next-Generation {YARN}},
  volume = {26},
  copyright = {Copyright Information Today, Inc. Dec 2012},
  issn = {1547-9897},
  url = {http://search.proquest.com.ezproxy.lib.monash.edu.au/docview/1265616176?accountid=12528},
  abstract = {As the undisputed pioneer of big data, Google established most of the key technologies underlying Hadoop and many of the {NoSQL} databases. The Google File System ({GFS}) allowed dusters of commodity servers to present their internal disk storage as a unified file system and inspired the Hadoop Distributed File System ({HDFS}). Google's column-oriented key value store {BigTable} influenced many {NoSQL} systems such as Apache {HBase}, Cassandra and {HyperTable}. And, of course, the Google {MapReduce} algorithm became the foundation computing model for Hadoop and was widely implemented in other {NoSQL} systems such as {MongoDB}.
{YARN} provides much more than just improved scalability, however; it treats traditional {MapReduce} as just one of the possible frameworks that can run on the cluster. {YARN}, therefore, will allow Hadoop clusters to execute non-{MapReduce} workloads. Work is underway on a number of such applications, including Spark, Storm, Giraph, and Hama.
Only the very largest Hadoop users are likely to benefit from {YARN}'s scalability improvements. For the rest of us, the ability to extend the range of Hadoop analytics is far more significant. At the end of the day, a Hadoop cluster is only as valuable as the analytic insights it can provide. By extending the range of possible analytic models, {YARN} should contribute to the long-term success of Hadoop.},
  language = {English},
  number = {4},
  urldate = {2014-08-11},
  journal = {Database Trends and Applications},
  author = {Harrison, Guy},
  month = dec,
  year = {2012},
  keywords = {Computers, Data bases, Product development, {SQL}},
  pages = {39}
}

@article{kamburugamuve_survey_2014,
  title = {Survey of Distributed Stream Processing for Large Stream Sources},
  url = {http://www.sics.se/~amir/files/download/dic/2013%20-%20Survey%20of%20Distributed%20Stream%20Processing%20for%20Large%20Stream%20Sources.pdf},
  urldate = {2014-08-31},
  author = {Kamburugamuve, Supun and Fox, Geoffrey and Leake, David and Qiu, Judy},
  file = {[PDF] from sics.se:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/92IIH8C2/Kamburugamuve et al. - Survey of Distributed Stream Processing for Large .pdf:application/pdf},
  year = {2014}
}

@misc{web:Samza,
  title = {Samza},
  year = {2014},
  howpublished = {\mbox{\url{http://samza.incubator.apache.org}}}
}


@book{islam_cloud_2014,
  title = {A Cloud Based Platform for Big Data Science},
  url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:690525},
  abstract = {With the advent of cloud computing, resizable scalable infrastructures for data processing is now available to everyone. Software platforms and frameworks that support data intensive distributed ap ...},
  language = {eng},
  urldate = {2014-08-31},
  author = {Islam, Md Zahidul},
  year = {2014},
  note = {With the advent of cloud computing, resizable scalable infrastructures for data processing is now available to everyone. Software platforms and frameworks that support data intensive distributed ap ...},
  keywords = {Programvaruteknik, Software Engineering},
  file = {Snapshot:/Users/poltak/Library/Application Support/Zotero/Profiles/xrhso8df.default/zotero/storage/J7Q6THMQ/record.html:text/html}
}

@article{DBLP:journals/corr/XinCDGFS14,
  author    = {Reynold S. Xin and
               Daniel Crankshaw and
               Ankur Dave and
               Joseph E. Gonzalez and
               Michael J. Franklin and
               Ion Stoica},
  title     = {GraphX: Unifying Data-Parallel and Graph-Parallel Analytics},
  journal   = {CoRR},
  volume    = {abs/1402.2394},
  year      = {2014},
  ee        = {http://arxiv.org/abs/1402.2394},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}